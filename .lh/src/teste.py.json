{
    "sourceFile": "src/teste.py",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 19,
            "patches": [
                {
                    "date": 1701529782650,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                },
                {
                    "date": 1701727177609,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,3 +1,556 @@\n from airflow import DAG\r\n+from airflow.operators.python import PythonOperator, BranchPythonOperator\r\n+from airflow.operators.bash import BashOperator\r\n\\ No newline at end of file\n+import requests\r\n+from bs4 import BeautifulSoup\r\n+import math\r\n+import re\r\n from datetime import datetime\r\n-from airflow.operators.python import PythonOperator\n+import locale\r\n+from cryptography.fernet import Fernet\r\n+import json\r\n+import tempfile\r\n+# import pysftp\r\n+import os\r\n+import psycopg2\r\n+from dotenv import load_dotenv\r\n+import pandas as pd\r\n+\r\n+headers = {\r\n+    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n+}\r\n+pattern = re.compile(r'\\d+')\r\n+# load_dotenv()\r\n+\r\n+# # # Transform password into bytes\r\n+\r\n+# key_bytes = os.getenv('chave').encode('utf-8')\r\n+\r\n+# # Cryptography functions and database inputs\r\n+\r\n+\r\n+def extract_page_number(soup):\r\n+    url = f\"{soup}\"\r\n+    pages = requests.get(url, headers=headers)\r\n+    soup_content = BeautifulSoup(pages.content, 'html.parser')\r\n+\r\n+    pagination = soup_content.find('ul', class_='pagination pagination-lg')\r\n+    span = pagination.find_all('span')\r\n+    print(span)\r\n+    itens_number_text = span[2].text\r\n+    transform_itens_number = itens_number_text.replace(\".\", \"\")\r\n+\r\n+    try:\r\n+        list_of_integer_number = re.findall(pattern, transform_itens_number)\r\n+    except ValueError:\r\n+        print(\"Error to convert itens to integer.\")\r\n+        return None\r\n+    number_of_itens = int(list_of_integer_number[2])\r\n+    result = number_of_itens / 25\r\n+    last_page = math.ceil(result)\r\n+    return last_page\r\n+\r\n+# # Functions to encrypt data\r\n+# def encryption(name):\r\n+#     cipher_suite = Fernet(key_bytes)\r\n+#     print(cipher_suite)\r\n+#     encrypted_text = cipher_suite.encrypt(json.dumps(name, ensure_ascii=False).encode('utf-8'))\r\n+#     return encrypted_text\r\n+\r\n+# # Functions to decryption and loading\r\n+# def decryption_and_loading(encrypted_text,table,master_key):\r\n+\r\n+#     # SFTP Connection Settings\r\n+#     host = os.getenv('host')\r\n+#     username = os.getenv('username')\r\n+#     port = 22\r\n+#     password = os.getenv('password')\r\n+#     cnopts = pysftp.CnOpts()\r\n+#     cnopts.hostkeys = None  # Disable hostkey verification\r\n+\r\n+#     # Temp file\r\n+#     with tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as temp_file:\r\n+#         temp_filename = temp_file.name\r\n+#         temp_file.write(encrypted_text)\r\n+#         temp_file.seek(0)\r\n+#         temp_file.close()\r\n+\r\n+\r\n+#         with pysftp.Connection(host, username=username, password=password, port=port, cnopts=cnopts) as sftp:\r\n+#             remote_path_upload = f\"C:\\\\Users\\\\Lenovo\\\\Desktop\\\\Projeto Data WH\\\\data output\\\\{table}.txt\"\r\n+#             sftp.put(temp_filename, remote_path_upload)\r\n+\r\n+#             os.remove(temp_filename)\r\n+\r\n+#             # Download a file from the SFTP server\r\n+#             remote_path_download = f\"C:\\\\Users\\\\Lenovo\\\\Desktop\\\\Projeto Data WH\\\\data output\\\\{table}.txt\"\r\n+#             local_path_download = 'remote_file_path_to_download.txt'\r\n+#             sftp.get(remote_path_download, local_path_download)\r\n+\r\n+\r\n+#     # Decrypt encrypted file data\r\n+#     with open(local_path_download, 'rb') as encrypted_file:\r\n+#         encrypted_date = encrypted_file.read().decode('utf-8')\r\n+#         cipher_suite = Fernet(key_bytes)\r\n+#         decrypt_text = cipher_suite.decrypt(encrypted_date)\r\n+#         decrypt_date = decrypt_text.decode('utf-8')\r\n+#         decrypt_dictionary = json.loads(decrypt_date)\r\n+\r\n+\r\n+#     keys = []\r\n+#     values = []\r\n+\r\n+#     for dictionary in decrypt_dictionary:\r\n+#         for key, value in dictionary.items():\r\n+#             keys.append(key)\r\n+#             values.append(value)\r\n+\r\n+#     db_config = {\r\n+#         'host': os.getenv('DBhost'),\r\n+#         'database': os.getenv('DBdatabase'),\r\n+#         'user': os.getenv('DBuser'),\r\n+#         'password': os.getenv('DBpassword')\r\n+#     }\r\n+\r\n+#     # Create a connection to the PostgreSQL database\r\n+#     try:\r\n+#         connection = psycopg2.connect(**db_config)\r\n+#         cursor = connection.cursor()\r\n+\r\n+#         # Query the database for existing keys\r\n+#         sql_query = f\"SELECT {master_key} FROM {table};\"\r\n+\r\n+#         # Execute the SQL query\r\n+#         cursor.execute(sql_query)\r\n+#         date_of_database = cursor.fetchall()\r\n+\r\n+#         # Run SQL query to get the unique keys that exist in the database\r\n+#         desired_key = f'{master_key}'\r\n+\r\n+#         existing_keys = [value1 for tupla in date_of_database for value1 in tupla]\r\n+#         existing_keys = set(existing_keys)\r\n+\r\n+\r\n+#         new_keys = {dictionary[desired_key] for dictionary in decrypt_dictionary}\r\n+#         new_keys = {int(number) for number in new_keys}\r\n+\r\n+\r\n+#         # Import only keys that do not already exist in the database\r\n+#         keys_new = new_keys - existing_keys\r\n+#         keys_new = list(keys_new)\r\n+\r\n+#         data_to_insert = []\r\n+\r\n+#         for dictionary in decrypt_dictionary:\r\n+#             # Checks if the desired key is present in the dictionary\r\n+#             if int(dictionary[desired_key]) in keys_new:\r\n+#                 # Add the complete dictionary to the list\r\n+#                 data_to_insert.append(dictionary)\r\n+#         print(data_to_insert)\r\n+\r\n+\r\n+#         try:\r\n+#             for dictionary_insert in data_to_insert:\r\n+#                 sql_query = f\"INSERT INTO {table} ({', '.join(dictionary_insert.keys())}) VALUES ({', '.join(['%s' for _ in dictionary_insert.values()])})\"\r\n+\r\n+#                 tupla_values = tuple(dictionary_insert.values())\r\n+\r\n+#                 # Execute the SQL query\r\n+#                 cursor.execute(sql_query,tupla_values)\r\n+\r\n+#                 # Commit to confirm the transaction\r\n+#                 connection.commit()\r\n+#                 print(\"Successful insertion.\")\r\n+#         except Exception as e:\r\n+#             print(\"Error executing SQL query:\", e)\r\n+\r\n+#     except Exception as e:\r\n+#         print(\"Error executing SQL query:\", e)\r\n+\r\n+\r\n+#     finally:\r\n+#         # Close the database connection\r\n+#         if connection:\r\n+#             cursor.close()\r\n+#             connection.close()\r\n+\r\n+#     return decrypt_dictionary\r\n+\r\n+\r\n+# # # players_information\r\n+\r\n+# last_page = extract_page_number(\"https://www.cbf.com.br/futebol-brasileiro/atletas/campeonato-brasileiro-serie-a/2023?atleta=&page=1&csrt=3199419627270262597\")\r\n+\r\n+def extract_players_informations_cbf(pages):\r\n+    players_informations = []\r\n+\r\n+    for number_page in range(1, pages + 1):\r\n+        url = f\"https://www.cbf.com.br/futebol-brasileiro/atletas/campeonato-brasileiro-serie-a/2023?atleta=&page={number_page}&csrt=5141281007868528954\"\r\n+        headers = {\r\n+            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n+        }\r\n+\r\n+        try:\r\n+            site = requests.get(url, headers=headers)\r\n+            soup = BeautifulSoup(site.content, \"html.parser\")\r\n+            body = soup.find('tbody')\r\n+            lines = body.find_all('tr')\r\n+\r\n+            for line in lines:\r\n+                columns = line.find_all('a')\r\n+                name_columns = columns[0]\r\n+                link = name_columns['href']\r\n+                id_player = re.findall(pattern, link)\r\n+                img = line.find('img')\r\n+                title = \"title nor found\" if img is None else img['alt']\r\n+\r\n+                players_informations.append({\r\n+                    'id_players': id_player[0],\r\n+                    'name': columns[0].text.strip(),\r\n+                    'nickname': columns[1].text.strip(),\r\n+                    'team': title,\r\n+                    'year': id_player[1]\r\n+                })\r\n+\r\n+        except Exception as e:\r\n+            print(f\"Error on page {number_page}: {str(e)}\")\r\n+\r\n+    return players_informations\r\n+\r\n+\r\n+# players_informations = extract_players_informations_cbf(last_page)\r\n+\r\n+\r\n+# # Transform and Load\r\n+# ## Encriptify/descriptify and input players information\r\n+\r\n+# players_informations_encriptify = encryption(players_informations)\r\n+\r\n+# players_informations_descriptify_and_input = decryption_and_loading(players_informations_encriptify,'relational.players_informations','id_players')\r\n+\r\n+# # # team_informations\r\n+# def extract_teams_informations_cbf():\r\n+#     teams_informations = []\r\n+#     try:\r\n+#         url_team = \"https://www.cbf.com.br/futebol-brasileiro/times/campeonato-brasileiro-serie-a/2023\"\r\n+#         site = requests.get(url_team, headers=headers)\r\n+#         soup_times = BeautifulSoup(site.content, \"html.parser\")\r\n+#         row = soup_times.find_all('div',class_='col-md-3 p-10')\r\n+\r\n+#         for time in row:\r\n+#             a = time.find('a')\r\n+#             link = a['href']\r\n+#             img = a.find('img')\r\n+#             name = img['alt']\r\n+#             team_reference = re.findall(pattern, link)\r\n+\r\n+#             if name != 'Esporte Clube Bahia - BA' and name != 'Atlético Mineiro - MG':\r\n+#                 teams_informations.append({\r\n+#                 'id_team': team_reference[1],\r\n+#                 'name': name,\r\n+#                 'year':team_reference[0]\r\n+#                 })\r\n+#     except Exception as e:\r\n+#         print(f\"Error on page: {str(e)}\")\r\n+#     return teams_informations\r\n+\r\n+# teams_informations = extract_teams_informations_cbf()\r\n+\r\n+# print(teams_informations)\r\n+\r\n+# # Transform and Load\r\n+# ## Encriptify/descriptify and input teams information\r\n+\r\n+# teams_informations_encriptify = encryption(teams_informations)\r\n+\r\n+# teams_informations_descriptify_and_input = decryption_and_loading(teams_informations_encriptify,'relational.teams_informations','id_team')\r\n+\r\n+# # # referees_informations\r\n+\r\n+# last_page = extract_page_number(\"https://www.cbf.com.br/a-cbf/arbitragem/relacao-arbitros\")\r\n+\r\n+\r\n+def extract_referees_informations_cbf(pages):\r\n+    referees_informations = []\r\n+\r\n+    for number_page in range(1, pages + 1):\r\n+        url = f\"https://www.cbf.com.br/a-cbf/arbitragem/relacao-arbitros?p=&l=&f=0&c=0&j=0&page={number_page}\"\r\n+        headers = {\r\n+            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n+        }\r\n+\r\n+        try:\r\n+            site = requests.get(url, headers=headers)\r\n+            soup = BeautifulSoup(site.content, \"html.parser\")\r\n+            body = soup.find('tbody')\r\n+            lines = body.find_all('tr')\r\n+\r\n+            for line in lines:\r\n+                columns = line.find_all('td')\r\n+                columns_name = columns[0]\r\n+                a = columns_name.find('a')\r\n+                link = a['href']\r\n+                id_referees = re.findall(pattern, link)\r\n+                name = columns[0].find('b')\r\n+                function = columns[1]\r\n+                federation = columns[2]\r\n+\r\n+                referees_informations.append({\r\n+                    'id_referees': id_referees[0],\r\n+                    'name': name.text.strip(),\r\n+                    'function': function.text.strip(),\r\n+                    'federation': federation.text.strip()\r\n+                })\r\n+\r\n+        except Exception as e:\r\n+            print(f\"Error on page {number_page}: {str(e)}\")\r\n+\r\n+    return referees_informations\r\n+\r\n+\r\n+# referees_informations = extract_referees_informations_cbf(last_page)\r\n+# for info in referees_informations:\r\n+#     print(info)\r\n+\r\n+# # # Transform and Load\r\n+# # ## Encriptify/descriptify and input referees information\r\n+\r\n+# referees_informations_encriptify = encryption(referees_informations)\r\n+\r\n+# referees_informations_descriptify_input = decryption_and_loading(referees_informations_encriptify,'relational.referees_informations','id_referees')\r\n+\r\n+# # # referees statistics\r\n+\r\n+# def extract_referees_statistics_cbf(pages):\r\n+#     referees_statistics = []\r\n+#     id_referees_statistics = 0\r\n+#     for number_page in range(1, pages + 1):\r\n+#         url = f\"https://www.cbf.com.br/futebol-brasileiro/competicoes/campeonato-brasileiro-serie-a/2023/{number_page}\"\r\n+#         headers = {\r\n+#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n+#         }\r\n+#         try:\r\n+#             site = requests.get(url, headers=headers)\r\n+#             soup = BeautifulSoup(site.content, \"html.parser\")\r\n+#             body = soup.find('tbody')\r\n+#             lines = body.find_all('tr')\r\n+\r\n+\r\n+#             for line in lines:\r\n+#                 a = line.find('a')\r\n+#                 link = a['href']\r\n+#                 id_referees = re.findall(pattern,link)\r\n+#                 functions = line.find('th')\r\n+#                 columns = line.find_all('td')\r\n+#                 name = columns[0]\r\n+#                 federation = columns[1]\r\n+#                 id_referees_statistics +=1\r\n+\r\n+\r\n+#                 referees_statistics.append({\r\n+#                     'id_referees_statistics': id_referees_statistics,\r\n+#                     'id_match': number_page,\r\n+#                     'id_referees': id_referees,\r\n+#                     'functions': functions.text.strip(),\r\n+#                     'name': name.text.strip(),\r\n+#                     'federation': federation.text.strip()\r\n+#                 })\r\n+#                 print(referees_statistics)\r\n+#         except Exception as e:\r\n+#             print(f\"Error on page {number_page}: {str(e)}\")\r\n+\r\n+#     return referees_statistics\r\n+\r\n+\r\n+# referees_statistics = extract_referees_statistics_cbf(380)\r\n+# print(referees_statistics)\r\n+\r\n+\r\n+# # # Transform and Load\r\n+# # ## Encriptify/descriptify and input referees statistic\r\n+\r\n+# referees_statistics_encriptify = encryption(referees_statistics)\r\n+\r\n+# referees_statistics_descriptify_and_input = decryption_and_loading(referees_statistics_encriptify,'relational.referees_statistics','id_match')\r\n+\r\n+# # # players statistics\r\n+\r\n+# def extract_players_statistics(pages):\r\n+#     players_statistics = []\r\n+#     id_players_statistics = 0\r\n+\r\n+#     for number_page in range(1, pages + 1):\r\n+#         url = f\"https://www.cbf.com.br/futebol-brasileiro/competicoes/campeonato-brasileiro-serie-a/2023/{number_page}#escalacao\"\r\n+#         headers = {\r\n+#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n+#         }\r\n+#         try:\r\n+#             site = requests.get(url, headers=headers)\r\n+#             soup = BeautifulSoup(site.content, \"html.parser\")\r\n+#             lineup = soup.find('div', class_='col-xs-12 col-sm-8 col-md-8 col-lg-8')\r\n+#             columns = lineup.find_all('ul', class_=re.compile(r'\\blist list-unstyled\\b'))\r\n+\r\n+#             for index_column, column in enumerate(columns):\r\n+\r\n+#                 if column.find(\"li\", string=\"Lineup not released\"):\r\n+#                     continue\r\n+#                 line_li = column.find_all('li')\r\n+\r\n+#                 if line_li:\r\n+#                     for li in line_li:\r\n+#                         role = 'Starting' if index_column < 2 else 'Reserve'\r\n+#                         players = li.find('a').text.strip()\r\n+#                         icon = li.find_all('i')\r\n+#                         goal = 0\r\n+#                         own_goal = 0\r\n+#                         yellow_card = 0\r\n+#                         red_card = 0\r\n+#                         time = []\r\n+#                         minutes = []\r\n+#                         id_players_statistics += 1\r\n+\r\n+#                         for i in icon:\r\n+#                             classes_i = i.get('class')\r\n+\r\n+#                             if 'small' in classes_i and 'icon' in classes_i and 'icon-yellow-card' not in classes_i:\r\n+#                                 goal += 1\r\n+#                                 time_of_match = i['title']\r\n+#                                 list_time_of_match = re.findall(pattern,time_of_match)\r\n+\r\n+#                                 if list_time_of_match:\r\n+#                                     if len(list_time_of_match) == 3:\r\n+#                                         time = int(list_time_of_match[2])\r\n+#                                         minutes = int(list_time_of_match[0]) + int(list_time_of_match[1])\r\n+\r\n+#                                     else:\r\n+#                                         time = int(list_time_of_match[1])\r\n+#                                         minutes = int(list_time_of_match[0])\r\n+#                                     time.append(time)\r\n+#                                     minutes.append(minutes)\r\n+#                             if 'small' in classes_i and 'icon' in classes_i and 'icon-yellow-card' in classes_i:\r\n+#                                 yellow_card +=1\r\n+#                             if 'small' in classes_i and 'icon' in classes_i and 'icon-red-card' in classes_i:\r\n+#                                 red_card +=1\r\n+\r\n+#                         div_list_phrese = soup.find('div', class_ =\"col-xs-3 col-sm-3 text-left hidden-xs\")\r\n+#                         list_phrase = div_list_phrese.find_all('p',class_='time-jogador color-red')\r\n+#                         if list_phrase:\r\n+#                             for list_ in list_phrase:\r\n+#                                 list_player_name_match = re.match(r'\\w+', list_.text.strip())\r\n+#                                 if list_player_name_match:\r\n+#                                     list_player_name = list_player_name_match.group()\r\n+#                                     list_player_name_split = list_player_name.split()\r\n+#                                     for player_name in list_player_name_split:\r\n+#                                         list_player = players.split()\r\n+#                                         for players in list_player:\r\n+#                                             if players == player_name:\r\n+#                                                 own_goal = 1\r\n+#                                             else:\r\n+#                                                 own_goal = 0\r\n+#                                 else:\r\n+#                                     list_player_name = None\r\n+#                         goal-=own_goal\r\n+\r\n+#                         players_statistics.append({\r\n+#                             'id_players_statistic':id_players_statistics,\r\n+#                             'id_match': number_page,\r\n+#                             'id_players': players,\r\n+#                             'role': role,\r\n+#                             'goal': goal,\r\n+#                             'own_goal': own_goal,\r\n+#                             'time': time,\r\n+#                             'minutes': minutes,\r\n+#                             'yellow_card': yellow_card,\r\n+#                             'red_card': red_card\r\n+#                         })\r\n+#                 else:\r\n+#                     print(\"No players were found in the column. Continuing in the next column....\")\r\n+#         except Exception as e:\r\n+#             print(f\"Error on page {number_page}: {str(e)}\")\r\n+#     return players_statistics\r\n+\r\n+# players_statistics = extract_players_statistics(380)\r\n+# print(players_statistics)\r\n+\r\n+# # # Transform and Load\r\n+# # ## Encriptify/descriptify and input players statistic\r\n+\r\n+# players_statistics_encriptify = encryption(players_statistics)\r\n+\r\n+# players_statistics_descriptify_and_input = decryption_and_loading(players_statistics_encriptify,'relational.players_statistics','id_match')\r\n+\r\n+# def extract_match_results_cbf(pages):\r\n+#     match_results = []\r\n+#     id_match_results = 0\r\n+\r\n+#     for number_page in range(1, pages + 1):\r\n+#         url = f\"https://www.cbf.com.br/futebol-brasileiro/competicoes/campeonato-brasileiro-serie-a/2023/{number_page}#escalacao\"\r\n+#         headers = {\r\n+#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n+#         }\r\n+#         try:\r\n+#             site = requests.get(url, headers=headers)\r\n+#             soup = BeautifulSoup(site.content, \"html.parser\")\r\n+#             match_data = soup.find('div', class_='col-sm-8')\r\n+#             match_data_elements = match_data.find_all('span', class_ =\"text-2 p-r-20\" )\r\n+#             scoreboard = soup.find('div', class_='placar-wrapper')\r\n+#             strong = scoreboard.find_all('strong', class_='time-gols block')\r\n+\r\n+#             # Lista para armazenar os gols das duas equipes\r\n+#             teams_goals = [int(team.text.strip()) for team in strong]\r\n+\r\n+\r\n+#             for i, teams in enumerate(strong):\r\n+#                 team = i+1\r\n+#                 if i == 0:\r\n+#                     id_match_results = (number_page*2)-1\r\n+#                 elif i == 1:\r\n+#                     id_match_results = (number_page*2)\r\n+#                 # Nome\r\n+#                 name = scoreboard.find('h3', class_='time-nome color-white').text.strip()\r\n+\r\n+#                 # Pontos inicializados como 0\r\n+#                 score = 0\r\n+\r\n+#                 # Lógica para atribuir pontos com base na quantidade de gols\r\n+#                 if teams_goals[0] > teams_goals[1]:\r\n+#                     score = 3 if i == 0 else 0\r\n+#                 elif teams_goals[0] < teams_goals[1]:\r\n+#                     score = 3 if i == 1 else 0\r\n+#                 else:\r\n+#                     score = 1\r\n+\r\n+#                 for index_elements, elements in enumerate(match_data_elements):\r\n+#                     if index_elements == 0:\r\n+#                         local = elements.text.strip()\r\n+#                     elif index_elements == 1:\r\n+#                         date_complete = elements.text.strip()\r\n+#                         match = re.search(r',(.*)', date_complete)\r\n+#                         date_original = match.group(1).strip()\r\n+#                         locale.setlocale(locale.LC_TIME, 'pt_BR.UTF-8')\r\n+#                         date_obj = datetime.strptime(date_original, \"%d de %B de %Y\")\r\n+#                         date = date_obj.strftime(\"%d/%m/%Y\")\r\n+\r\n+#                 match_results.append({\r\n+#                     'id_match_result': id_match_results,\r\n+#                     'id_match': number_page,\r\n+#                     'local': local,\r\n+#                     'date': date,\r\n+#                     'team': team,\r\n+#                     'name': name,\r\n+#                     'goal': teams_goals[i],  # Gols da equipe i\r\n+#                     'score': score\r\n+#                 })\r\n+#         except Exception as e:\r\n+#             print(f\"Error on page {number_page}: {str(e)}\")\r\n+#     return match_results\r\n+\r\n+# match_results = extract_match_results_cbf(380)\r\n+\r\n+\r\n+# # # Transform and Load\r\n+# # ## Encriptify/descriptify and input match results\r\n+\r\n+# match_results_encriptify = encryption(match_results)\r\n+\r\n+# match_results_descriptify_and_input = decryption_and_loading(match_results_encriptify,'relational.matchs_results','id_match')\n\\ No newline at end of file\n"
                },
                {
                    "date": 1701727183203,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,7 +1,5 @@\n-from airflow import DAG\r\n-from airflow.operators.python import PythonOperator, BranchPythonOperator\r\n-from airflow.operators.bash import BashOperator\r\n+\r\n import requests\r\n from bs4 import BeautifulSoup\r\n import math\r\n import re\r\n"
                },
                {
                    "date": 1701727195801,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -176,9 +176,9 @@\n \r\n \r\n # # # players_information\r\n \r\n-# last_page = extract_page_number(\"https://www.cbf.com.br/futebol-brasileiro/atletas/campeonato-brasileiro-serie-a/2023?atleta=&page=1&csrt=3199419627270262597\")\r\n+last_page = extract_page_number(\"https://www.cbf.com.br/futebol-brasileiro/atletas/campeonato-brasileiro-serie-a/2023?atleta=&page=1&csrt=3199419627270262597\")\r\n \r\n def extract_players_informations_cbf(pages):\r\n     players_informations = []\r\n \r\n"
                },
                {
                    "date": 1701727203829,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -215,9 +215,9 @@\n \r\n     return players_informations\r\n \r\n \r\n-# players_informations = extract_players_informations_cbf(last_page)\r\n+players_informations = extract_players_informations_cbf(last_page)\r\n \r\n \r\n # # Transform and Load\r\n # ## Encriptify/descriptify and input players information\r\n"
                },
                {
                    "date": 1701727210484,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -216,8 +216,9 @@\n     return players_informations\r\n \r\n \r\n players_informations = extract_players_informations_cbf(last_page)\r\n+print\r\n \r\n \r\n # # Transform and Load\r\n # ## Encriptify/descriptify and input players information\r\n"
                },
                {
                    "date": 1701727216024,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -216,9 +216,9 @@\n     return players_informations\r\n \r\n \r\n players_informations = extract_players_informations_cbf(last_page)\r\n-print\r\n+print(players_informations)\r\n \r\n \r\n # # Transform and Load\r\n # ## Encriptify/descriptify and input players information\r\n"
                },
                {
                    "date": 1701727254427,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -11,9 +11,9 @@\n # import pysftp\r\n import os\r\n import psycopg2\r\n from dotenv import load_dotenv\r\n-import pandas as pd\r\n+#import pandas as pd\r\n \r\n headers = {\r\n     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n }\r\n"
                },
                {
                    "date": 1702419707922,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -7,549 +7,46 @@\n import locale\r\n from cryptography.fernet import Fernet\r\n import json\r\n import tempfile\r\n-# import pysftp\r\n import os\r\n import psycopg2\r\n-from dotenv import load_dotenv\r\n-#import pandas as pd\r\n+from requests.adapters import HTTPAdapter\r\n+from urllib3.util.retry import Retry\r\n \r\n+\r\n headers = {\r\n     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n }\r\n pattern = re.compile(r'\\d+')\r\n-# load_dotenv()\r\n \r\n-# # # Transform password into bytes\r\n+session = requests.Session()\r\n+retry = Retry(connect=3, backoff_factor=0.5)\r\n+adapter = HTTPAdapter(max_retries=retry)\r\n+session.mount('http://', adapter)\r\n+session.mount('https://', adapter)\r\n \r\n-# key_bytes = os.getenv('chave').encode('utf-8')\r\n \r\n-# # Cryptography functions and database inputs\r\n-\r\n-\r\n-def extract_page_number(soup):\r\n-    url = f\"{soup}\"\r\n-    pages = requests.get(url, headers=headers)\r\n-    soup_content = BeautifulSoup(pages.content, 'html.parser')\r\n-\r\n-    pagination = soup_content.find('ul', class_='pagination pagination-lg')\r\n-    span = pagination.find_all('span')\r\n-    print(span)\r\n-    itens_number_text = span[2].text\r\n-    transform_itens_number = itens_number_text.replace(\".\", \"\")\r\n-\r\n+def extract_teams_informations_cbf():\r\n+    teams_informations = []\r\n     try:\r\n-        list_of_integer_number = re.findall(pattern, transform_itens_number)\r\n-    except ValueError:\r\n-        print(\"Error to convert itens to integer.\")\r\n-        return None\r\n-    number_of_itens = int(list_of_integer_number[2])\r\n-    result = number_of_itens / 25\r\n-    last_page = math.ceil(result)\r\n-    return last_page\r\n+        url_team = \"https://www.cbf.com.br/futebol-brasileiro/times/campeonato-brasileiro-serie-a/2023\"\r\n+        site = session.get(url_team, headers=headers)\r\n+        soup_times = BeautifulSoup(site.content, \"html.parser\")\r\n+        row = soup_times.find_all('div', class_='col-md-3 p-10')\r\n \r\n-# # Functions to encrypt data\r\n-# def encryption(name):\r\n-#     cipher_suite = Fernet(key_bytes)\r\n-#     print(cipher_suite)\r\n-#     encrypted_text = cipher_suite.encrypt(json.dumps(name, ensure_ascii=False).encode('utf-8'))\r\n-#     return encrypted_text\r\n+        for time in row:\r\n+            a = time.find('a')\r\n+            link = a['href']\r\n+            img = a.find('img')\r\n+            name = img['alt']\r\n+            team_reference = re.findall(pattern, link)\r\n \r\n-# # Functions to decryption and loading\r\n-# def decryption_and_loading(encrypted_text,table,master_key):\r\n-\r\n-#     # SFTP Connection Settings\r\n-#     host = os.getenv('host')\r\n-#     username = os.getenv('username')\r\n-#     port = 22\r\n-#     password = os.getenv('password')\r\n-#     cnopts = pysftp.CnOpts()\r\n-#     cnopts.hostkeys = None  # Disable hostkey verification\r\n-\r\n-#     # Temp file\r\n-#     with tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as temp_file:\r\n-#         temp_filename = temp_file.name\r\n-#         temp_file.write(encrypted_text)\r\n-#         temp_file.seek(0)\r\n-#         temp_file.close()\r\n-\r\n-\r\n-#         with pysftp.Connection(host, username=username, password=password, port=port, cnopts=cnopts) as sftp:\r\n-#             remote_path_upload = f\"C:\\\\Users\\\\Lenovo\\\\Desktop\\\\Projeto Data WH\\\\data output\\\\{table}.txt\"\r\n-#             sftp.put(temp_filename, remote_path_upload)\r\n-\r\n-#             os.remove(temp_filename)\r\n-\r\n-#             # Download a file from the SFTP server\r\n-#             remote_path_download = f\"C:\\\\Users\\\\Lenovo\\\\Desktop\\\\Projeto Data WH\\\\data output\\\\{table}.txt\"\r\n-#             local_path_download = 'remote_file_path_to_download.txt'\r\n-#             sftp.get(remote_path_download, local_path_download)\r\n-\r\n-\r\n-#     # Decrypt encrypted file data\r\n-#     with open(local_path_download, 'rb') as encrypted_file:\r\n-#         encrypted_date = encrypted_file.read().decode('utf-8')\r\n-#         cipher_suite = Fernet(key_bytes)\r\n-#         decrypt_text = cipher_suite.decrypt(encrypted_date)\r\n-#         decrypt_date = decrypt_text.decode('utf-8')\r\n-#         decrypt_dictionary = json.loads(decrypt_date)\r\n-\r\n-\r\n-#     keys = []\r\n-#     values = []\r\n-\r\n-#     for dictionary in decrypt_dictionary:\r\n-#         for key, value in dictionary.items():\r\n-#             keys.append(key)\r\n-#             values.append(value)\r\n-\r\n-#     db_config = {\r\n-#         'host': os.getenv('DBhost'),\r\n-#         'database': os.getenv('DBdatabase'),\r\n-#         'user': os.getenv('DBuser'),\r\n-#         'password': os.getenv('DBpassword')\r\n-#     }\r\n-\r\n-#     # Create a connection to the PostgreSQL database\r\n-#     try:\r\n-#         connection = psycopg2.connect(**db_config)\r\n-#         cursor = connection.cursor()\r\n-\r\n-#         # Query the database for existing keys\r\n-#         sql_query = f\"SELECT {master_key} FROM {table};\"\r\n-\r\n-#         # Execute the SQL query\r\n-#         cursor.execute(sql_query)\r\n-#         date_of_database = cursor.fetchall()\r\n-\r\n-#         # Run SQL query to get the unique keys that exist in the database\r\n-#         desired_key = f'{master_key}'\r\n-\r\n-#         existing_keys = [value1 for tupla in date_of_database for value1 in tupla]\r\n-#         existing_keys = set(existing_keys)\r\n-\r\n-\r\n-#         new_keys = {dictionary[desired_key] for dictionary in decrypt_dictionary}\r\n-#         new_keys = {int(number) for number in new_keys}\r\n-\r\n-\r\n-#         # Import only keys that do not already exist in the database\r\n-#         keys_new = new_keys - existing_keys\r\n-#         keys_new = list(keys_new)\r\n-\r\n-#         data_to_insert = []\r\n-\r\n-#         for dictionary in decrypt_dictionary:\r\n-#             # Checks if the desired key is present in the dictionary\r\n-#             if int(dictionary[desired_key]) in keys_new:\r\n-#                 # Add the complete dictionary to the list\r\n-#                 data_to_insert.append(dictionary)\r\n-#         print(data_to_insert)\r\n-\r\n-\r\n-#         try:\r\n-#             for dictionary_insert in data_to_insert:\r\n-#                 sql_query = f\"INSERT INTO {table} ({', '.join(dictionary_insert.keys())}) VALUES ({', '.join(['%s' for _ in dictionary_insert.values()])})\"\r\n-\r\n-#                 tupla_values = tuple(dictionary_insert.values())\r\n-\r\n-#                 # Execute the SQL query\r\n-#                 cursor.execute(sql_query,tupla_values)\r\n-\r\n-#                 # Commit to confirm the transaction\r\n-#                 connection.commit()\r\n-#                 print(\"Successful insertion.\")\r\n-#         except Exception as e:\r\n-#             print(\"Error executing SQL query:\", e)\r\n-\r\n-#     except Exception as e:\r\n-#         print(\"Error executing SQL query:\", e)\r\n-\r\n-\r\n-#     finally:\r\n-#         # Close the database connection\r\n-#         if connection:\r\n-#             cursor.close()\r\n-#             connection.close()\r\n-\r\n-#     return decrypt_dictionary\r\n-\r\n-\r\n-# # # players_information\r\n-\r\n-last_page = extract_page_number(\"https://www.cbf.com.br/futebol-brasileiro/atletas/campeonato-brasileiro-serie-a/2023?atleta=&page=1&csrt=3199419627270262597\")\r\n-\r\n-def extract_players_informations_cbf(pages):\r\n-    players_informations = []\r\n-\r\n-    for number_page in range(1, pages + 1):\r\n-        url = f\"https://www.cbf.com.br/futebol-brasileiro/atletas/campeonato-brasileiro-serie-a/2023?atleta=&page={number_page}&csrt=5141281007868528954\"\r\n-        headers = {\r\n-            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n-        }\r\n-\r\n-        try:\r\n-            site = requests.get(url, headers=headers)\r\n-            soup = BeautifulSoup(site.content, \"html.parser\")\r\n-            body = soup.find('tbody')\r\n-            lines = body.find_all('tr')\r\n-\r\n-            for line in lines:\r\n-                columns = line.find_all('a')\r\n-                name_columns = columns[0]\r\n-                link = name_columns['href']\r\n-                id_player = re.findall(pattern, link)\r\n-                img = line.find('img')\r\n-                title = \"title nor found\" if img is None else img['alt']\r\n-\r\n-                players_informations.append({\r\n-                    'id_players': id_player[0],\r\n-                    'name': columns[0].text.strip(),\r\n-                    'nickname': columns[1].text.strip(),\r\n-                    'team': title,\r\n-                    'year': id_player[1]\r\n+            if name != 'Esporte Clube Bahia - BA' and name != 'Atlético Mineiro - MG':\r\n+                teams_informations.append({\r\n+                    'id_team': team_reference[1],\r\n+                    'name': name,\r\n+                    'year': team_reference[0]\r\n                 })\r\n-\r\n-        except Exception as e:\r\n-            print(f\"Error on page {number_page}: {str(e)}\")\r\n-\r\n-    return players_informations\r\n-\r\n-\r\n-players_informations = extract_players_informations_cbf(last_page)\r\n-print(players_informations)\r\n-\r\n-\r\n-# # Transform and Load\r\n-# ## Encriptify/descriptify and input players information\r\n-\r\n-# players_informations_encriptify = encryption(players_informations)\r\n-\r\n-# players_informations_descriptify_and_input = decryption_and_loading(players_informations_encriptify,'relational.players_informations','id_players')\r\n-\r\n-# # # team_informations\r\n-# def extract_teams_informations_cbf():\r\n-#     teams_informations = []\r\n-#     try:\r\n-#         url_team = \"https://www.cbf.com.br/futebol-brasileiro/times/campeonato-brasileiro-serie-a/2023\"\r\n-#         site = requests.get(url_team, headers=headers)\r\n-#         soup_times = BeautifulSoup(site.content, \"html.parser\")\r\n-#         row = soup_times.find_all('div',class_='col-md-3 p-10')\r\n-\r\n-#         for time in row:\r\n-#             a = time.find('a')\r\n-#             link = a['href']\r\n-#             img = a.find('img')\r\n-#             name = img['alt']\r\n-#             team_reference = re.findall(pattern, link)\r\n-\r\n-#             if name != 'Esporte Clube Bahia - BA' and name != 'Atlético Mineiro - MG':\r\n-#                 teams_informations.append({\r\n-#                 'id_team': team_reference[1],\r\n-#                 'name': name,\r\n-#                 'year':team_reference[0]\r\n-#                 })\r\n-#     except Exception as e:\r\n-#         print(f\"Error on page: {str(e)}\")\r\n-#     return teams_informations\r\n-\r\n-# teams_informations = extract_teams_informations_cbf()\r\n-\r\n-# print(teams_informations)\r\n-\r\n-# # Transform and Load\r\n-# ## Encriptify/descriptify and input teams information\r\n-\r\n-# teams_informations_encriptify = encryption(teams_informations)\r\n-\r\n-# teams_informations_descriptify_and_input = decryption_and_loading(teams_informations_encriptify,'relational.teams_informations','id_team')\r\n-\r\n-# # # referees_informations\r\n-\r\n-# last_page = extract_page_number(\"https://www.cbf.com.br/a-cbf/arbitragem/relacao-arbitros\")\r\n-\r\n-\r\n-def extract_referees_informations_cbf(pages):\r\n-    referees_informations = []\r\n-\r\n-    for number_page in range(1, pages + 1):\r\n-        url = f\"https://www.cbf.com.br/a-cbf/arbitragem/relacao-arbitros?p=&l=&f=0&c=0&j=0&page={number_page}\"\r\n-        headers = {\r\n-            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n-        }\r\n-\r\n-        try:\r\n-            site = requests.get(url, headers=headers)\r\n-            soup = BeautifulSoup(site.content, \"html.parser\")\r\n-            body = soup.find('tbody')\r\n-            lines = body.find_all('tr')\r\n-\r\n-            for line in lines:\r\n-                columns = line.find_all('td')\r\n-                columns_name = columns[0]\r\n-                a = columns_name.find('a')\r\n-                link = a['href']\r\n-                id_referees = re.findall(pattern, link)\r\n-                name = columns[0].find('b')\r\n-                function = columns[1]\r\n-                federation = columns[2]\r\n-\r\n-                referees_informations.append({\r\n-                    'id_referees': id_referees[0],\r\n-                    'name': name.text.strip(),\r\n-                    'function': function.text.strip(),\r\n-                    'federation': federation.text.strip()\r\n-                })\r\n-\r\n-        except Exception as e:\r\n-            print(f\"Error on page {number_page}: {str(e)}\")\r\n-\r\n-    return referees_informations\r\n-\r\n-\r\n-# referees_informations = extract_referees_informations_cbf(last_page)\r\n-# for info in referees_informations:\r\n-#     print(info)\r\n-\r\n-# # # Transform and Load\r\n-# # ## Encriptify/descriptify and input referees information\r\n-\r\n-# referees_informations_encriptify = encryption(referees_informations)\r\n-\r\n-# referees_informations_descriptify_input = decryption_and_loading(referees_informations_encriptify,'relational.referees_informations','id_referees')\r\n-\r\n-# # # referees statistics\r\n-\r\n-# def extract_referees_statistics_cbf(pages):\r\n-#     referees_statistics = []\r\n-#     id_referees_statistics = 0\r\n-#     for number_page in range(1, pages + 1):\r\n-#         url = f\"https://www.cbf.com.br/futebol-brasileiro/competicoes/campeonato-brasileiro-serie-a/2023/{number_page}\"\r\n-#         headers = {\r\n-#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n-#         }\r\n-#         try:\r\n-#             site = requests.get(url, headers=headers)\r\n-#             soup = BeautifulSoup(site.content, \"html.parser\")\r\n-#             body = soup.find('tbody')\r\n-#             lines = body.find_all('tr')\r\n-\r\n-\r\n-#             for line in lines:\r\n-#                 a = line.find('a')\r\n-#                 link = a['href']\r\n-#                 id_referees = re.findall(pattern,link)\r\n-#                 functions = line.find('th')\r\n-#                 columns = line.find_all('td')\r\n-#                 name = columns[0]\r\n-#                 federation = columns[1]\r\n-#                 id_referees_statistics +=1\r\n-\r\n-\r\n-#                 referees_statistics.append({\r\n-#                     'id_referees_statistics': id_referees_statistics,\r\n-#                     'id_match': number_page,\r\n-#                     'id_referees': id_referees,\r\n-#                     'functions': functions.text.strip(),\r\n-#                     'name': name.text.strip(),\r\n-#                     'federation': federation.text.strip()\r\n-#                 })\r\n-#                 print(referees_statistics)\r\n-#         except Exception as e:\r\n-#             print(f\"Error on page {number_page}: {str(e)}\")\r\n-\r\n-#     return referees_statistics\r\n-\r\n-\r\n-# referees_statistics = extract_referees_statistics_cbf(380)\r\n-# print(referees_statistics)\r\n-\r\n-\r\n-# # # Transform and Load\r\n-# # ## Encriptify/descriptify and input referees statistic\r\n-\r\n-# referees_statistics_encriptify = encryption(referees_statistics)\r\n-\r\n-# referees_statistics_descriptify_and_input = decryption_and_loading(referees_statistics_encriptify,'relational.referees_statistics','id_match')\r\n-\r\n-# # # players statistics\r\n-\r\n-# def extract_players_statistics(pages):\r\n-#     players_statistics = []\r\n-#     id_players_statistics = 0\r\n-\r\n-#     for number_page in range(1, pages + 1):\r\n-#         url = f\"https://www.cbf.com.br/futebol-brasileiro/competicoes/campeonato-brasileiro-serie-a/2023/{number_page}#escalacao\"\r\n-#         headers = {\r\n-#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n-#         }\r\n-#         try:\r\n-#             site = requests.get(url, headers=headers)\r\n-#             soup = BeautifulSoup(site.content, \"html.parser\")\r\n-#             lineup = soup.find('div', class_='col-xs-12 col-sm-8 col-md-8 col-lg-8')\r\n-#             columns = lineup.find_all('ul', class_=re.compile(r'\\blist list-unstyled\\b'))\r\n-\r\n-#             for index_column, column in enumerate(columns):\r\n-\r\n-#                 if column.find(\"li\", string=\"Lineup not released\"):\r\n-#                     continue\r\n-#                 line_li = column.find_all('li')\r\n-\r\n-#                 if line_li:\r\n-#                     for li in line_li:\r\n-#                         role = 'Starting' if index_column < 2 else 'Reserve'\r\n-#                         players = li.find('a').text.strip()\r\n-#                         icon = li.find_all('i')\r\n-#                         goal = 0\r\n-#                         own_goal = 0\r\n-#                         yellow_card = 0\r\n-#                         red_card = 0\r\n-#                         time = []\r\n-#                         minutes = []\r\n-#                         id_players_statistics += 1\r\n-\r\n-#                         for i in icon:\r\n-#                             classes_i = i.get('class')\r\n-\r\n-#                             if 'small' in classes_i and 'icon' in classes_i and 'icon-yellow-card' not in classes_i:\r\n-#                                 goal += 1\r\n-#                                 time_of_match = i['title']\r\n-#                                 list_time_of_match = re.findall(pattern,time_of_match)\r\n-\r\n-#                                 if list_time_of_match:\r\n-#                                     if len(list_time_of_match) == 3:\r\n-#                                         time = int(list_time_of_match[2])\r\n-#                                         minutes = int(list_time_of_match[0]) + int(list_time_of_match[1])\r\n-\r\n-#                                     else:\r\n-#                                         time = int(list_time_of_match[1])\r\n-#                                         minutes = int(list_time_of_match[0])\r\n-#                                     time.append(time)\r\n-#                                     minutes.append(minutes)\r\n-#                             if 'small' in classes_i and 'icon' in classes_i and 'icon-yellow-card' in classes_i:\r\n-#                                 yellow_card +=1\r\n-#                             if 'small' in classes_i and 'icon' in classes_i and 'icon-red-card' in classes_i:\r\n-#                                 red_card +=1\r\n-\r\n-#                         div_list_phrese = soup.find('div', class_ =\"col-xs-3 col-sm-3 text-left hidden-xs\")\r\n-#                         list_phrase = div_list_phrese.find_all('p',class_='time-jogador color-red')\r\n-#                         if list_phrase:\r\n-#                             for list_ in list_phrase:\r\n-#                                 list_player_name_match = re.match(r'\\w+', list_.text.strip())\r\n-#                                 if list_player_name_match:\r\n-#                                     list_player_name = list_player_name_match.group()\r\n-#                                     list_player_name_split = list_player_name.split()\r\n-#                                     for player_name in list_player_name_split:\r\n-#                                         list_player = players.split()\r\n-#                                         for players in list_player:\r\n-#                                             if players == player_name:\r\n-#                                                 own_goal = 1\r\n-#                                             else:\r\n-#                                                 own_goal = 0\r\n-#                                 else:\r\n-#                                     list_player_name = None\r\n-#                         goal-=own_goal\r\n-\r\n-#                         players_statistics.append({\r\n-#                             'id_players_statistic':id_players_statistics,\r\n-#                             'id_match': number_page,\r\n-#                             'id_players': players,\r\n-#                             'role': role,\r\n-#                             'goal': goal,\r\n-#                             'own_goal': own_goal,\r\n-#                             'time': time,\r\n-#                             'minutes': minutes,\r\n-#                             'yellow_card': yellow_card,\r\n-#                             'red_card': red_card\r\n-#                         })\r\n-#                 else:\r\n-#                     print(\"No players were found in the column. Continuing in the next column....\")\r\n-#         except Exception as e:\r\n-#             print(f\"Error on page {number_page}: {str(e)}\")\r\n-#     return players_statistics\r\n-\r\n-# players_statistics = extract_players_statistics(380)\r\n-# print(players_statistics)\r\n-\r\n-# # # Transform and Load\r\n-# # ## Encriptify/descriptify and input players statistic\r\n-\r\n-# players_statistics_encriptify = encryption(players_statistics)\r\n-\r\n-# players_statistics_descriptify_and_input = decryption_and_loading(players_statistics_encriptify,'relational.players_statistics','id_match')\r\n-\r\n-# def extract_match_results_cbf(pages):\r\n-#     match_results = []\r\n-#     id_match_results = 0\r\n-\r\n-#     for number_page in range(1, pages + 1):\r\n-#         url = f\"https://www.cbf.com.br/futebol-brasileiro/competicoes/campeonato-brasileiro-serie-a/2023/{number_page}#escalacao\"\r\n-#         headers = {\r\n-#             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n-#         }\r\n-#         try:\r\n-#             site = requests.get(url, headers=headers)\r\n-#             soup = BeautifulSoup(site.content, \"html.parser\")\r\n-#             match_data = soup.find('div', class_='col-sm-8')\r\n-#             match_data_elements = match_data.find_all('span', class_ =\"text-2 p-r-20\" )\r\n-#             scoreboard = soup.find('div', class_='placar-wrapper')\r\n-#             strong = scoreboard.find_all('strong', class_='time-gols block')\r\n-\r\n-#             # Lista para armazenar os gols das duas equipes\r\n-#             teams_goals = [int(team.text.strip()) for team in strong]\r\n-\r\n-\r\n-#             for i, teams in enumerate(strong):\r\n-#                 team = i+1\r\n-#                 if i == 0:\r\n-#                     id_match_results = (number_page*2)-1\r\n-#                 elif i == 1:\r\n-#                     id_match_results = (number_page*2)\r\n-#                 # Nome\r\n-#                 name = scoreboard.find('h3', class_='time-nome color-white').text.strip()\r\n-\r\n-#                 # Pontos inicializados como 0\r\n-#                 score = 0\r\n-\r\n-#                 # Lógica para atribuir pontos com base na quantidade de gols\r\n-#                 if teams_goals[0] > teams_goals[1]:\r\n-#                     score = 3 if i == 0 else 0\r\n-#                 elif teams_goals[0] < teams_goals[1]:\r\n-#                     score = 3 if i == 1 else 0\r\n-#                 else:\r\n-#                     score = 1\r\n-\r\n-#                 for index_elements, elements in enumerate(match_data_elements):\r\n-#                     if index_elements == 0:\r\n-#                         local = elements.text.strip()\r\n-#                     elif index_elements == 1:\r\n-#                         date_complete = elements.text.strip()\r\n-#                         match = re.search(r',(.*)', date_complete)\r\n-#                         date_original = match.group(1).strip()\r\n-#                         locale.setlocale(locale.LC_TIME, 'pt_BR.UTF-8')\r\n-#                         date_obj = datetime.strptime(date_original, \"%d de %B de %Y\")\r\n\\ No newline at end of file\n-#                         date = date_obj.strftime(\"%d/%m/%Y\")\r\n-\r\n-#                 match_results.append({\r\n-#                     'id_match_result': id_match_results,\r\n-#                     'id_match': number_page,\r\n-#                     'local': local,\r\n-#                     'date': date,\r\n-#                     'team': team,\r\n-#                     'name': name,\r\n-#                     'goal': teams_goals[i],  # Gols da equipe i\r\n-#                     'score': score\r\n-#                 })\r\n-#         except Exception as e:\r\n-#             print(f\"Error on page {number_page}: {str(e)}\")\r\n-#     return match_results\r\n-\r\n-# match_results = extract_match_results_cbf(380)\r\n-\r\n-\r\n-# # # Transform and Load\r\n-# # ## Encriptify/descriptify and input match results\r\n-\r\n-# match_results_encriptify = encryption(match_results)\r\n-\r\n-# match_results_descriptify_and_input = decryption_and_loading(match_results_encriptify,'relational.matchs_results','id_match')\n+    except Exception as e:\r\n+        print(f\"Error on page: {str(e)}\")\r\n+    return teams_informations\n\\ No newline at end of file\n"
                },
                {
                    "date": 1703882470856,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -25,28 +25,111 @@\n session.mount('http://', adapter)\r\n session.mount('https://', adapter)\r\n \r\n \r\n-def extract_teams_informations_cbf():\r\n-    teams_informations = []\r\n-    try:\r\n-        url_team = \"https://www.cbf.com.br/futebol-brasileiro/times/campeonato-brasileiro-serie-a/2023\"\r\n-        site = session.get(url_team, headers=headers)\r\n-        soup_times = BeautifulSoup(site.content, \"html.parser\")\r\n-        row = soup_times.find_all('div', class_='col-md-3 p-10')\r\n+def extract_players_statistics_cbf():\r\n+    players_statistics = []\r\n+    id_players_statistics = 0\r\n \r\n-        for time in row:\r\n-            a = time.find('a')\r\n-            link = a['href']\r\n-            img = a.find('img')\r\n-            name = img['alt']\r\n-            team_reference = re.findall(pattern, link)\r\n+    for number_page in range(1, 380 + 1):\r\n+        url = f\"https://www.cbf.com.br/futebol-brasileiro/competicoes/campeonato-brasileiro-serie-a/2023/{number_page}#escalacao\"\r\n+        headers = {\r\n+            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n+        }\r\n+        try:\r\n+            site = requests.get(url, headers=headers)\r\n\\ No newline at end of file\n+            soup = BeautifulSoup(site.content, \"html.parser\")\r\n+            lineup = soup.find(\r\n+                'div', class_='col-xs-12 col-sm-8 col-md-8 col-lg-8')\r\n+            columns = lineup.find_all(\r\n+                'ul', class_=re.compile(r'\\blist list-unstyled\\b'))\r\n \r\n-            if name != 'Esporte Clube Bahia - BA' and name != 'Atlético Mineiro - MG':\r\n-                teams_informations.append({\r\n-                    'id_team': team_reference[1],\r\n-                    'name': name,\r\n-                    'year': team_reference[0]\r\n-                })\r\n-    except Exception as e:\r\n-        print(f\"Error on page: {str(e)}\")\r\n-    return teams_informations\n+            for index_column, column in enumerate(columns):\r\n+\r\n+                if column.find(\"li\", string=\"Lineup not released\"):\r\n+                    continue\r\n+                line_li = column.find_all('li')\r\n+\r\n+                if line_li:\r\n+                    for li in line_li:\r\n+                        role = 'Starting' if index_column < 2 else 'Reserve'\r\n+                        players = li.find('a')\r\n+                        if players:\r\n+                            players.text.strip()\r\n+                        icon = li.find_all('i')\r\n+                        goal = 0\r\n+                        own_goal = 0\r\n+                        yellow_card = 0\r\n+                        red_card = 0\r\n+                        time = []\r\n+                        minutes = []\r\n+                        id_players_statistics += 1\r\n+\r\n+                        for i in icon:\r\n+                            classes_i = i.get('class')\r\n+\r\n+                            if 'small' in classes_i and 'icon' in classes_i and 'icon-yellow-card' not in classes_i:\r\n+                                goal += 1\r\n+                                if 'title' in i.attrs:\r\n+                                    time_of_match = i['title']\r\n+                                    list_time_of_match = re.findall(\r\n+                                        pattern, time_of_match)\r\n+\r\n+                                    if list_time_of_match:\r\n+                                        if len(list_time_of_match) == 3:\r\n+                                            time_val = int(\r\n+                                                list_time_of_match[2])\r\n+                                            minutes_val = int(\r\n+                                                list_time_of_match[0]) + int(list_time_of_match[1])\r\n+\r\n+                                        else:\r\n+                                            time_val = int(\r\n+                                                list_time_of_match[1])\r\n+                                            minutes_val = int(\r\n+                                                list_time_of_match[0])\r\n+                                        time.append(time_val)\r\n+                                        minutes.append(minutes_val)\r\n+                            if 'small' in classes_i and 'icon' in classes_i and 'icon-yellow-card' in classes_i:\r\n+                                yellow_card += 1\r\n+                            if 'small' in classes_i and 'icon' in classes_i and 'icon-red-card' in classes_i:\r\n+                                red_card += 1\r\n+\r\n+                        div_list_phrese = soup.find(\r\n+                            'div', class_=\"col-xs-3 col-sm-3 text-left hidden-xs\")\r\n+                        list_phrase = div_list_phrese.find_all(\r\n+                            'p', class_='time-jogador color-red')\r\n+                        if list_phrase:\r\n+                            for list_ in list_phrase:\r\n+                                list_player_name_match = re.match(\r\n+                                    r'\\w+', list_.text.strip())\r\n+                                if list_player_name_match:\r\n+                                    list_player_name = list_player_name_match.group()\r\n+                                    list_player_name_split = list_player_name.split()\r\n+                                    for player_name in list_player_name_split:\r\n+                                        list_player = players.split()\r\n+                                        for players in list_player:\r\n+                                            if players == player_name:\r\n+                                                own_goal = 1\r\n+                                            else:\r\n+                                                own_goal = 0\r\n+                                else:\r\n+                                    list_player_name = None\r\n+                        goal -= own_goal\r\n+\r\n+                        players_statistics.append({\r\n+                            'id_players_statistic': id_players_statistics,\r\n+                            'id_match': number_page,\r\n+                            'id_players': players,\r\n+                            'role': role,\r\n+                            'goal': goal,\r\n+                            'own_goal': own_goal,\r\n+                            'time': time,\r\n+                            'minutes': minutes,\r\n+                            'yellow_card': yellow_card,\r\n+                            'red_card': red_card\r\n+                        })\r\n+                else:\r\n+                    print(\r\n+                        \"No players were found in the column. Continuing in the next column....\")\r\n+        except Exception as e:\r\n+            print(f\"Error on page {number_page}: {str(e)}\")\r\n+    return players_statistics\n\\ No newline at end of file\n"
                },
                {
                    "date": 1703882575681,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -131,5 +131,6 @@\n                     print(\r\n                         \"No players were found in the column. Continuing in the next column....\")\r\n         except Exception as e:\r\n             print(f\"Error on page {number_page}: {str(e)}\")\r\n-    return players_statistics\n\\ No newline at end of file\n+    return players_statistics\r\n+\r\n"
                },
                {
                    "date": 1703882582326,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -133,4 +133,5 @@\n         except Exception as e:\r\n             print(f\"Error on page {number_page}: {str(e)}\")\r\n     return players_statistics\r\n \r\n+print(players_statistics)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1703882588978,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -133,5 +133,5 @@\n         except Exception as e:\r\n             print(f\"Error on page {number_page}: {str(e)}\")\r\n     return players_statistics\r\n \r\n-print(players_statistics)\n\\ No newline at end of file\n+    print(players_statistics)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1703882594601,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -133,5 +133,6 @@\n         except Exception as e:\r\n             print(f\"Error on page {number_page}: {str(e)}\")\r\n     return players_statistics\r\n \r\n+for \r\n     print(players_statistics)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1703882601995,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -133,6 +133,6 @@\n         except Exception as e:\r\n             print(f\"Error on page {number_page}: {str(e)}\")\r\n     return players_statistics\r\n \r\n-for \r\n+for info in \r\n     print(players_statistics)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1703882608455,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -133,6 +133,6 @@\n         except Exception as e:\r\n             print(f\"Error on page {number_page}: {str(e)}\")\r\n     return players_statistics\r\n \r\n-for info in \r\n+for info in players_statis\r\n     print(players_statistics)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1703882614731,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -0,0 +1,139 @@\n+\r\n+import requests\r\n+from bs4 import BeautifulSoup\r\n+import math\r\n+import re\r\n+from datetime import datetime\r\n+import locale\r\n+from cryptography.fernet import Fernet\r\n+import json\r\n+import tempfile\r\n+import os\r\n+import psycopg2\r\n+from requests.adapters import HTTPAdapter\r\n+from urllib3.util.retry import Retry\r\n+\r\n+\r\n+headers = {\r\n+    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n+}\r\n+pattern = re.compile(r'\\d+')\r\n+\r\n+session = requests.Session()\r\n+retry = Retry(connect=3, backoff_factor=0.5)\r\n+adapter = HTTPAdapter(max_retries=retry)\r\n+session.mount('http://', adapter)\r\n+session.mount('https://', adapter)\r\n+\r\n+\r\n+def extract_players_statistics_cbf():\r\n+    players_statistics = []\r\n+    id_players_statistics = 0\r\n+\r\n+    for number_page in range(1, 380 + 1):\r\n+        url = f\"https://www.cbf.com.br/futebol-brasileiro/competicoes/campeonato-brasileiro-serie-a/2023/{number_page}#escalacao\"\r\n+        headers = {\r\n+            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n+        }\r\n+        try:\r\n+            site = requests.get(url, headers=headers)\r\n+            soup = BeautifulSoup(site.content, \"html.parser\")\r\n+            lineup = soup.find(\r\n+                'div', class_='col-xs-12 col-sm-8 col-md-8 col-lg-8')\r\n+            columns = lineup.find_all(\r\n+                'ul', class_=re.compile(r'\\blist list-unstyled\\b'))\r\n+\r\n+            for index_column, column in enumerate(columns):\r\n+\r\n+                if column.find(\"li\", string=\"Lineup not released\"):\r\n+                    continue\r\n+                line_li = column.find_all('li')\r\n+\r\n+                if line_li:\r\n+                    for li in line_li:\r\n+                        role = 'Starting' if index_column < 2 else 'Reserve'\r\n+                        players = li.find('a')\r\n+                        if players:\r\n+                            players.text.strip()\r\n+                        icon = li.find_all('i')\r\n+                        goal = 0\r\n+                        own_goal = 0\r\n+                        yellow_card = 0\r\n+                        red_card = 0\r\n+                        time = []\r\n+                        minutes = []\r\n+                        id_players_statistics += 1\r\n+\r\n+                        for i in icon:\r\n+                            classes_i = i.get('class')\r\n+\r\n+                            if 'small' in classes_i and 'icon' in classes_i and 'icon-yellow-card' not in classes_i:\r\n+                                goal += 1\r\n+                                if 'title' in i.attrs:\r\n+                                    time_of_match = i['title']\r\n+                                    list_time_of_match = re.findall(\r\n+                                        pattern, time_of_match)\r\n+\r\n+                                    if list_time_of_match:\r\n+                                        if len(list_time_of_match) == 3:\r\n+                                            time_val = int(\r\n+                                                list_time_of_match[2])\r\n+                                            minutes_val = int(\r\n+                                                list_time_of_match[0]) + int(list_time_of_match[1])\r\n+\r\n+                                        else:\r\n+                                            time_val = int(\r\n+                                                list_time_of_match[1])\r\n+                                            minutes_val = int(\r\n+                                                list_time_of_match[0])\r\n+                                        time.append(time_val)\r\n+                                        minutes.append(minutes_val)\r\n+                            if 'small' in classes_i and 'icon' in classes_i and 'icon-yellow-card' in classes_i:\r\n+                                yellow_card += 1\r\n+                            if 'small' in classes_i and 'icon' in classes_i and 'icon-red-card' in classes_i:\r\n+                                red_card += 1\r\n+\r\n+                        div_list_phrese = soup.find(\r\n+                            'div', class_=\"col-xs-3 col-sm-3 text-left hidden-xs\")\r\n+                        list_phrase = div_list_phrese.find_all(\r\n+                            'p', class_='time-jogador color-red')\r\n+                        if list_phrase:\r\n+                            for list_ in list_phrase:\r\n+                                list_player_name_match = re.match(\r\n+                                    r'\\w+', list_.text.strip())\r\n+                                if list_player_name_match:\r\n+                                    list_player_name = list_player_name_match.group()\r\n+                                    list_player_name_split = list_player_name.split()\r\n+                                    for player_name in list_player_name_split:\r\n+                                        list_player = players.split()\r\n+                                        for players in list_player:\r\n+                                            if players == player_name:\r\n+                                                own_goal = 1\r\n+                                            else:\r\n+                                                own_goal = 0\r\n+                                else:\r\n+                                    list_player_name = None\r\n+                        goal -= own_goal\r\n+\r\n+                        players_statistics.append({\r\n+                            'id_players_statistic': id_players_statistics,\r\n+                            'id_match': number_page,\r\n+                            'id_players': players,\r\n+                            'role': role,\r\n+                            'goal': goal,\r\n+                            'own_goal': own_goal,\r\n+                            'time': time,\r\n+                            'minutes': minutes,\r\n+                            'yellow_card': yellow_card,\r\n+                            'red_card': red_card\r\n+                        })\r\n+                else:\r\n+                    print(\r\n+                        \"No players were found in the column. Continuing in the next column....\")\r\n+        except Exception as e:\r\n+            print(f\"Error on page {number_page}: {str(e)}\")\r\n+    return players_statistics\r\n+\r\n+for info in players_statistics:\r\n+    \r\n+    print(players_statistics)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1703882691403,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -133,145 +133,8 @@\n         except Exception as e:\r\n             print(f\"Error on page {number_page}: {str(e)}\")\r\n     return players_statistics\r\n \r\n+players_statistics = \r\n+\r\n for info in players_statistics:\r\n-    \r\n-    print(players_statistics)\n-\r\n-import requests\r\n-from bs4 import BeautifulSoup\r\n-import math\r\n-import re\r\n-from datetime import datetime\r\n-import locale\r\n-from cryptography.fernet import Fernet\r\n-import json\r\n-import tempfile\r\n-import os\r\n-import psycopg2\r\n-from requests.adapters import HTTPAdapter\r\n-from urllib3.util.retry import Retry\r\n-\r\n-\r\n-headers = {\r\n-    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n-}\r\n-pattern = re.compile(r'\\d+')\r\n-\r\n-session = requests.Session()\r\n-retry = Retry(connect=3, backoff_factor=0.5)\r\n-adapter = HTTPAdapter(max_retries=retry)\r\n-session.mount('http://', adapter)\r\n-session.mount('https://', adapter)\r\n-\r\n-\r\n-def extract_players_statistics_cbf():\r\n-    players_statistics = []\r\n-    id_players_statistics = 0\r\n-\r\n-    for number_page in range(1, 380 + 1):\r\n-        url = f\"https://www.cbf.com.br/futebol-brasileiro/competicoes/campeonato-brasileiro-serie-a/2023/{number_page}#escalacao\"\r\n-        headers = {\r\n-            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n-        }\r\n-        try:\r\n-            site = requests.get(url, headers=headers)\r\n-            soup = BeautifulSoup(site.content, \"html.parser\")\r\n-            lineup = soup.find(\r\n-                'div', class_='col-xs-12 col-sm-8 col-md-8 col-lg-8')\r\n-            columns = lineup.find_all(\r\n-                'ul', class_=re.compile(r'\\blist list-unstyled\\b'))\r\n-\r\n-            for index_column, column in enumerate(columns):\r\n-\r\n-                if column.find(\"li\", string=\"Lineup not released\"):\r\n-                    continue\r\n-                line_li = column.find_all('li')\r\n-\r\n-                if line_li:\r\n-                    for li in line_li:\r\n-                        role = 'Starting' if index_column < 2 else 'Reserve'\r\n-                        players = li.find('a')\r\n-                        if players:\r\n-                            players.text.strip()\r\n-                        icon = li.find_all('i')\r\n-                        goal = 0\r\n-                        own_goal = 0\r\n-                        yellow_card = 0\r\n-                        red_card = 0\r\n-                        time = []\r\n-                        minutes = []\r\n-                        id_players_statistics += 1\r\n-\r\n-                        for i in icon:\r\n-                            classes_i = i.get('class')\r\n-\r\n-                            if 'small' in classes_i and 'icon' in classes_i and 'icon-yellow-card' not in classes_i:\r\n-                                goal += 1\r\n-                                if 'title' in i.attrs:\r\n-                                    time_of_match = i['title']\r\n-                                    list_time_of_match = re.findall(\r\n-                                        pattern, time_of_match)\r\n-\r\n-                                    if list_time_of_match:\r\n-                                        if len(list_time_of_match) == 3:\r\n-                                            time_val = int(\r\n-                                                list_time_of_match[2])\r\n-                                            minutes_val = int(\r\n-                                                list_time_of_match[0]) + int(list_time_of_match[1])\r\n-\r\n-                                        else:\r\n-                                            time_val = int(\r\n-                                                list_time_of_match[1])\r\n-                                            minutes_val = int(\r\n-                                                list_time_of_match[0])\r\n-                                        time.append(time_val)\r\n-                                        minutes.append(minutes_val)\r\n-                            if 'small' in classes_i and 'icon' in classes_i and 'icon-yellow-card' in classes_i:\r\n-                                yellow_card += 1\r\n-                            if 'small' in classes_i and 'icon' in classes_i and 'icon-red-card' in classes_i:\r\n-                                red_card += 1\r\n-\r\n-                        div_list_phrese = soup.find(\r\n-                            'div', class_=\"col-xs-3 col-sm-3 text-left hidden-xs\")\r\n-                        list_phrase = div_list_phrese.find_all(\r\n-                            'p', class_='time-jogador color-red')\r\n-                        if list_phrase:\r\n-                            for list_ in list_phrase:\r\n-                                list_player_name_match = re.match(\r\n-                                    r'\\w+', list_.text.strip())\r\n-                                if list_player_name_match:\r\n-                                    list_player_name = list_player_name_match.group()\r\n-                                    list_player_name_split = list_player_name.split()\r\n-                                    for player_name in list_player_name_split:\r\n-                                        list_player = players.split()\r\n-                                        for players in list_player:\r\n-                                            if players == player_name:\r\n-                                                own_goal = 1\r\n-                                            else:\r\n-                                                own_goal = 0\r\n-                                else:\r\n-                                    list_player_name = None\r\n-                        goal -= own_goal\r\n-\r\n-                        players_statistics.append({\r\n-                            'id_players_statistic': id_players_statistics,\r\n-                            'id_match': number_page,\r\n-                            'id_players': players,\r\n-                            'role': role,\r\n-                            'goal': goal,\r\n-                            'own_goal': own_goal,\r\n-                            'time': time,\r\n-                            'minutes': minutes,\r\n-                            'yellow_card': yellow_card,\r\n-                            'red_card': red_card\r\n-                        })\r\n-                else:\r\n-                    print(\r\n-                        \"No players were found in the column. Continuing in the next column....\")\r\n-        except Exception as e:\r\n-            print(f\"Error on page {number_page}: {str(e)}\")\r\n-    return players_statistics\r\n-\r\n-for info in players_statis\r\n     print(players_statistics)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1703882710997,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -133,8 +133,8 @@\n         except Exception as e:\r\n             print(f\"Error on page {number_page}: {str(e)}\")\r\n     return players_statistics\r\n \r\n-players_statistics = \r\n+players_statistics = extract_players_statistics_cbf()\r\n \r\n for info in players_statistics:\r\n     print(players_statistics)\n\\ No newline at end of file\n"
                },
                {
                    "date": 1704226476011,
                    "content": "Index: \n===================================================================\n--- \n+++ \n@@ -1,5 +1,4 @@\n-\r\n import requests\r\n from bs4 import BeautifulSoup\r\n import math\r\n import re\r\n@@ -9,8 +8,9 @@\n import json\r\n import tempfile\r\n import os\r\n import psycopg2\r\n+import time as tm\r\n from requests.adapters import HTTPAdapter\r\n from urllib3.util.retry import Retry\r\n \r\n \r\n@@ -28,10 +28,10 @@\n \r\n def extract_players_statistics_cbf():\r\n     players_statistics = []\r\n     id_players_statistics = 0\r\n-\r\n-    for number_page in range(1, 380 + 1):\r\n+    list_player = []\r\n+    for number_page in range(1, 380+1):\r\n         url = f\"https://www.cbf.com.br/futebol-brasileiro/competicoes/campeonato-brasileiro-serie-a/2023/{number_page}#escalacao\"\r\n         headers = {\r\n             'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36'\r\n         }\r\n@@ -43,9 +43,8 @@\n             columns = lineup.find_all(\r\n                 'ul', class_=re.compile(r'\\blist list-unstyled\\b'))\r\n \r\n             for index_column, column in enumerate(columns):\r\n-\r\n                 if column.find(\"li\", string=\"Lineup not released\"):\r\n                     continue\r\n                 line_li = column.find_all('li')\r\n \r\n@@ -53,18 +52,18 @@\n                     for li in line_li:\r\n                         role = 'Starting' if index_column < 2 else 'Reserve'\r\n                         players = li.find('a')\r\n                         if players:\r\n-                            players.text.strip()\r\n+                            players = players.text.strip() \r\n                         icon = li.find_all('i')\r\n                         goal = 0\r\n                         own_goal = 0\r\n                         yellow_card = 0\r\n                         red_card = 0\r\n                         time = []\r\n                         minutes = []\r\n                         id_players_statistics += 1\r\n-\r\n+                         \r\n                         for i in icon:\r\n                             classes_i = i.get('class')\r\n \r\n                             if 'small' in classes_i and 'icon' in classes_i and 'icon-yellow-card' not in classes_i:\r\n@@ -91,9 +90,8 @@\n                             if 'small' in classes_i and 'icon' in classes_i and 'icon-yellow-card' in classes_i:\r\n                                 yellow_card += 1\r\n                             if 'small' in classes_i and 'icon' in classes_i and 'icon-red-card' in classes_i:\r\n                                 red_card += 1\r\n-\r\n                         div_list_phrese = soup.find(\r\n                             'div', class_=\"col-xs-3 col-sm-3 text-left hidden-xs\")\r\n                         list_phrase = div_list_phrese.find_all(\r\n                             'p', class_='time-jogador color-red')\r\n@@ -103,19 +101,19 @@\n                                     r'\\w+', list_.text.strip())\r\n                                 if list_player_name_match:\r\n                                     list_player_name = list_player_name_match.group()\r\n                                     list_player_name_split = list_player_name.split()\r\n+                                    #aqui ja estou com a lista de pessoas com gol contra\r\n                                     for player_name in list_player_name_split:\r\n-                                        list_player = players.split()\r\n-                                        for players in list_player:\r\n-                                            if players == player_name:\r\n+                                        list_player.append(players)\r\n+                                        for players1 in list_player:\r\n+                                            if players1 == player_name:\r\n                                                 own_goal = 1\r\n                                             else:\r\n                                                 own_goal = 0\r\n                                 else:\r\n                                     list_player_name = None\r\n                         goal -= own_goal\r\n-\r\n                         players_statistics.append({\r\n                             'id_players_statistic': id_players_statistics,\r\n                             'id_match': number_page,\r\n                             'id_players': players,\r\n@@ -136,5 +134,153 @@\n \r\n players_statistics = extract_players_statistics_cbf()\r\n \r\n for info in players_statistics:\r\n-    print(players_statistics)\n\\ No newline at end of file\n+    print(info)\r\n+    \r\n+def encryption():\r\n+\r\n+    encrypted_bytes = cipher_suite.encrypt(json.dumps(\r\n+        source_task_result, ensure_ascii=False).encode('utf-8'))\r\n+\r\n+\r\n+    encrypted_str = base64.b64encode(encrypted_bytes).decode('utf-8')\r\n+    # ti.xcom_push_binary(key='encrypted_data', value=encrypted_bytes)\r\n+\r\n+    # SFTP Connection Settings\r\n+    host = '192.168.0.143'\r\n+    username = 'Lenovo'\r\n+    port = 22\r\n+    password = 'Fab@9492'\r\n+\r\n+    # encrypted_bytes = base64.b64decode(encrypted_text).encode('utf-8')\r\n+\r\n+    # Temp file\r\n+    with tempfile.NamedTemporaryFile(delete=False, suffix='.txt') as temp_file:\r\n+        temp_filename = temp_file.name\r\n+        temp_file.write(encrypted_bytes)\r\n+        temp_file.seek(0)\r\n+        temp_file.close()\r\n+\r\n+        transport = paramiko.Transport((host, port))\r\n+        transport.connect(username=username, password=password)\r\n+        sftp = paramiko.SFTPClient.from_transport(transport)\r\n+        remote_path_upload = f\"C:\\\\Users\\\\Lenovo\\\\Desktop\\\\Projeto Data WH\\\\data output\\\\{table}.txt\"\r\n+        sftp.put(temp_filename, remote_path_upload)\r\n+\r\n+        os.remove(temp_filename)\r\n+\r\n+    return encrypted_str\r\n+\r\n+\r\n+# Functions to decryption and loading\r\n+\r\n+\r\n+def decryption_and_loading(encrypted_text, table, master_key):\r\n+    print(key_bytes)\r\n+    # SFTP Connection Settings\r\n+    host = '192.168.0.143'\r\n+    username = 'Lenovo'\r\n+    port = 22\r\n+    password = 'Fab@9492'\r\n+\r\n+    # encrypted_bytes = base64.b64decode(encrypted_text).encode('utf-8')\r\n+\r\n+    # Download a file from the SFTP server\r\n+    transport = paramiko.Transport((host, port))\r\n+    transport.connect(username=username, password=password)\r\n+    sftp = paramiko.SFTPClient.from_transport(transport)\r\n+    remote_path_download = f\"C:\\\\Users\\\\Lenovo\\\\Desktop\\\\Projeto Data WH\\\\data output\\\\{table}.txt\"\r\n+    local_path_download = 'remote_file_path_to_download.txt'\r\n+    sftp.get(remote_path_download, local_path_download)\r\n+    sftp.close()\r\n+    transport.close()\r\n+\r\n+    # Decrypt encrypted file data\r\n+    with open(local_path_download, 'rb') as encrypted_file:\r\n+        encrypted_date = encrypted_file.read()\r\n+        print(encrypted_date)\r\n+        decrypt_text = cipher_suite.decrypt(encrypted_date)\r\n+        print(decrypt_text)\r\n+        decrypt_date = decrypt_text.decode('utf-8')\r\n+        print(decrypt_date)\r\n+        decrypt_date = decrypt_date.strip('\"')\r\n+        print(decrypt_date)\r\n+        decrypt_date_fixed = decrypt_date.replace(\"'\", '\"')\r\n+        decrypt_dictionary = json.loads(decrypt_date_fixed)\r\n+        print(decrypt_dictionary)\r\n+        tipo = type(decrypt_dictionary)\r\n+        print(tipo)\r\n+\r\n+    keys = []\r\n+    values = []\r\n+\r\n+    for dictionary in decrypt_dictionary:\r\n+        for key, value in dictionary.items():\r\n+            keys.append(key)\r\n+            values.append(value)\r\n+            \r\n+    db_config = {\r\n+        'conn_id': 'postgres-airflow',  # Use o Conn Id configurado no Airflow\r\n+    }\r\n+    # db_config = {\r\n+    #     'host': 'postgres',\r\n+    #     'database': 'Campeonato_Brasileiro_2023',\r\n+    #     'user': 'postgres',\r\n+    #     'password': 'Fabricio@94921321'}\r\n+\r\n+    # connection = psycopg2.connect(**db_config)\r\n+    connection = PostgresHook(postgres_conn_id=db_config['conn_id']).get_conn()\r\n+    cursor = connection.cursor()\r\n+\r\n+    # Query the database for existing keys\r\n+    sql_query = f'SELECT {master_key} FROM {table};'\r\n+\r\n+    # Execute the SQL query\r\n+    cursor.execute(sql_query)\r\n+    date_of_database = cursor.fetchall()\r\n+\r\n+    # Run SQL query to get the unique keys that exist in the database\r\n+    desired_key = f'{master_key}'\r\n+\r\n+    existing_keys = [\r\n+        value1 for tupla in date_of_database for value1 in tupla]\r\n+    existing_keys = set(existing_keys)\r\n+\r\n+    new_keys = {dictionary[desired_key]\r\n+                for dictionary in decrypt_dictionary}\r\n+    new_keys = {int(number) for number in new_keys}\r\n+\r\n+    # Import only keys that do not already exist in the database\r\n+    keys_new = new_keys - existing_keys\r\n+    keys_new = list(keys_new)\r\n+\r\n+    data_to_insert = []\r\n+\r\n+    for dictionary in decrypt_dictionary:\r\n+        # Checks if the desired key is present in the dictionary\r\n+        if int(dictionary[desired_key]) in keys_new:\r\n+            # Add the complete dictionary to the list\r\n+            data_to_insert.append(dictionary)\r\n+\r\n+    try:\r\n+        for dictionary_insert in data_to_insert:\r\n+            sql_query = f\"INSERT INTO {table} ({', '.join(dictionary_insert.keys())}) VALUES ({', '.join(['%s' for _ in dictionary_insert.values()])})\"\r\n+\r\n+            tupla_values = tuple(dictionary_insert.values())\r\n+\r\n+            # Execute the SQL query\r\n+            cursor.execute(sql_query, tupla_values)\r\n+\r\n+            # Commit to confirm the transaction\r\n+            connection.commit()\r\n+            print(\"Successful insertion.\")\r\n+    except Exception as e:\r\n+        print(\"Error executing SQL query:\", e)\r\n+\r\n+    finally:\r\n+        # Close the database connection\r\n+        if connection:\r\n+            cursor.close()\r\n+            connection.close()\r\n+\r\n+    return decrypt_dictionary\n\\ No newline at end of file\n"
                }
            ],
            "date": 1701529782650,
            "name": "Commit-0",
            "content": "from airflow import DAG\r\nfrom datetime import datetime\r\nfrom airflow.operators.python import PythonOperator"
        }
    ]
}